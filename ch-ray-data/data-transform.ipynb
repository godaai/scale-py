{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sec-ray-data-transform)=\n",
    "# Data Transformation\n",
    "\n",
    "The core of data processing is a series of transformations, including:\n",
    "\n",
    "* How to transform individual rows or batches of data.\n",
    "* How to perform grouping with `groupby`.\n",
    "\n",
    "## Transformation\n",
    "\n",
    "### `map()` and `map_batches()`\n",
    "\n",
    "Ray Data provides two types of data transformation operations, as shown in {numref}`map-map-batches`. These two transformation operations are typical examples of embarrassingly parallel computation, with no data shuffle overhead.\n",
    "\n",
    "* For each individual row, you can use [`Dataset.map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map.html) and [`Dataset.flat_map()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html). These APIs perform transformations on each individual row, similar to other big data frameworks like Spark or Flink. Input one row, output one row.\n",
    "* To package multiple rows into a batch and perform batch-wise transformations, you can use [`Dataset.map_batches()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html). Input one batch, output one batch.\n",
    "\n",
    "```{figure} ../img/ch-ray-data/map-map-batches.svg\n",
    "---\n",
    "width: 800px\n",
    "name: map-map-batches\n",
    "---\n",
    "`map()` v.s.`map_batches()`\n",
    "```\n",
    "\n",
    "### Example: NYC Taxi \n",
    "\n",
    "We will use the New York City taxi dataset to demonstrate how to use these two types of transformation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:27,440\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from datasets import nyc_taxi\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ray\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data into the `Dataset` class and first examine the original data format, where `tpep_pickup_datetime` and `tpep_dropoff_datetime` represent the passenger pickup and drop-off times, including both date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb7f934594b4bdaa3635eb2ff39081b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parquet Files Sample 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:28,931\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2024-02-15 19:10:28,933\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 200, each read task output is split into 50 smaller blocks.\n",
      "2024-02-15 19:10:28,933\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> LimitOperator[limit=1]\n",
      "2024-02-15 19:10:28,933\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:28,934\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce1515dbd2740c9b1ab9ab18b2052d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5836)\u001b[0m /Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:148: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5836)\u001b[0m   return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'VendorID': 1,\n",
       "  'tpep_pickup_datetime': datetime.datetime(2023, 6, 1, 0, 8, 48),\n",
       "  'tpep_dropoff_datetime': datetime.datetime(2023, 6, 1, 0, 29, 41),\n",
       "  'passenger_count': 1,\n",
       "  'trip_distance': 3.4,\n",
       "  'RatecodeID': 1,\n",
       "  'store_and_fwd_flag': 'N',\n",
       "  'PULocationID': 140,\n",
       "  'DOLocationID': 238,\n",
       "  'payment_type': 1,\n",
       "  'fare_amount': 21.9,\n",
       "  'extra': 3.5,\n",
       "  'mta_tax': 0.5,\n",
       "  'tip_amount': 6.7,\n",
       "  'tolls_amount': 0.0,\n",
       "  'improvement_surcharge': 1.0,\n",
       "  'total_amount': 33.6,\n",
       "  'congestion_surcharge': 2.5,\n",
       "  'Airport_fee': 0.0}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = nyc_taxi()\n",
    "dataset = ray.data.read_parquet(dataset_path)\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "All operations in Ray Data are lazy. Instead, they are executed when encountering data viewing or saving operations such as [`show()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.show.html), [`take()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.take.html), [`iter_rows()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.iter_rows.html), [`write_parquet()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.write_parquet.html), etc.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameter of `map(fn)` is a custom callable function `fn`, which transforms each input row and returns an output row. In this example, we defined the function `transform_row` to extract the duration, distance, and price of each trip, ignoring other fields. Since `map(fn)`'s `fn` is used to transform a single row of data, the input type of the function is a key-value `Dict`, where the keys are the field names of the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:29,496\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2024-02-15 19:10:29,497\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 200, each read task output is split into 50 smaller blocks.\n",
      "2024-02-15 19:10:29,497\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[Map(transform_row)] -> LimitOperator[limit=1]\n",
      "2024-02-15 19:10:29,497\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:29,497\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf71a82e0b84a0dbe84829f12c9e5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253.0, 'trip_distance': 3.4, 'fare_amount': 21.9}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    result = {}\n",
    "    result[\"trip_duration\"] = (row[\"tpep_dropoff_datetime\"] - row[\"tpep_pickup_datetime\"]).total_seconds()\n",
    "    result[\"trip_distance\"] = row[\"trip_distance\"]\n",
    "    result[\"fare_amount\"] = row[\"fare_amount\"]\n",
    "    return result\n",
    "\n",
    "row_ds = dataset.map(transform_row)\n",
    "row_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differing from `map()`, `map_batches()` processes an entire batch. The design philosophy behind this method is to facilitate the seamless migration of previously written single-node programs to Ray. Users can first develop a single-node program and then use Ray Data to move it to a cluster. The data format for each batch in `map_batches()` is `Dict[str, np.ndarray]` or `pd.DataFrame` or `pyarrow.Table`, corresponding to NumPy, pandas, and Arrow, respectively.\n",
    "\n",
    "The following example achieves similar functionality to `map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:31,936\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2024-02-15 19:10:31,936\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 200, each read task output is split into 50 smaller blocks.\n",
      "2024-02-15 19:10:31,936\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=10]\n",
      "2024-02-15 19:10:31,936\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:31,937\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10576103ca14b2d8cfbc8c97e10b5ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5829)\u001b[0m /Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:148: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5829)\u001b[0m   return transform_pyarrow.concat(tables)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'trip_duration': 1253, 'trip_distance': 3.4, 'fare_amount': 21.9},\n",
       " {'trip_duration': 614, 'trip_distance': 3.4, 'fare_amount': 15.6},\n",
       " {'trip_duration': 1123, 'trip_distance': 10.2, 'fare_amount': 40.8},\n",
       " {'trip_duration': 1406, 'trip_distance': 9.83, 'fare_amount': 39.4},\n",
       " {'trip_duration': 514, 'trip_distance': 1.17, 'fare_amount': 9.3},\n",
       " {'trip_duration': 796, 'trip_distance': 3.6, 'fare_amount': 18.4},\n",
       " {'trip_duration': 1136, 'trip_distance': 3.08, 'fare_amount': 19.8},\n",
       " {'trip_duration': 527, 'trip_distance': 1.1, 'fare_amount': 10.0},\n",
       " {'trip_duration': 237, 'trip_distance': 0.99, 'fare_amount': 6.5},\n",
       " {'trip_duration': 171, 'trip_distance': 0.73, 'fare_amount': 5.1}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"trip_duration\"] = (input_df[\"tpep_dropoff_datetime\"] - input_df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    result_df[\"trip_distance\"] = input_df[\"trip_distance\"]\n",
    "    result_df[\"fare_amount\"] = input_df[\"fare_amount\"]\n",
    "    return result_df\n",
    "\n",
    "batch_ds = dataset.map_batches(transform_df, batch_format=\"pandas\")\n",
    "batch_ds.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实现 `map()` 或者 `map_batch()` 时，也可以使用 Python 的 lambda 表达式，即一个匿名的 Python 函数。比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:32,499\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2024-02-15 19:10:32,499\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 200, each read task output is split into 50 smaller blocks.\n",
      "2024-02-15 19:10:32,500\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(<lambda>)]\n",
      "2024-02-15 19:10:32,500\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:32,500\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7b4c95159741ed8f73cc66743c4e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered: 730352\n"
     ]
    }
   ],
   "source": [
    "filtered_dataset = dataset.map_batches(lambda df: df[df[\"trip_distance\"] > 4], batch_format=\"pandas\")\n",
    "print(f\"Filtered: {filtered_dataset.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task and Actor\n",
    "\n",
    "As observed, the transformation operation involves executing `fn`. This function takes an input, performs the transformation, and produces an output. By default, Ray Data utilizes task for executing transformation operations. Ray Tasks are suitable for stateless computations. If the computation involves state, Ray Actors should be used. For instance, loading a machine learning model and using it to predict all data inputs. The following example simulates the process of predicting with a machine learning model. Since the model is reused, it involves stateful computation. This example is for demonstration purposes, and it uses an equivalent transformation, `torch.nn.Identity()`, which returns the input as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:33,990\tWARNING util.py:546 -- The argument ``compute`` is deprecated in Ray 2.9. Please specify argument ``concurrency`` instead. For more information, see https://docs.ray.io/en/master/data/transforming-data.html#stateful-transforms.\n",
      "2024-02-15 19:10:33,993\tINFO set_read_parallelism.py:115 -- Using autodetected parallelism=200 for stage ReadParquet to satisfy DataContext.get_current().min_parallelism=200.\n",
      "2024-02-15 19:10:33,993\tINFO set_read_parallelism.py:122 -- To satisfy the requested parallelism of 200, each read task output is split into 50 smaller blocks.\n",
      "2024-02-15 19:10:33,993\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> TaskPoolMapOperator[ReadParquet] -> TaskPoolMapOperator[MapBatches(transform_df)] -> LimitOperator[limit=100] -> ActorPoolMapOperator[MapBatches(TorchPredictor)] -> LimitOperator[limit=3]\n",
      "2024-02-15 19:10:33,994\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:33,994\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n",
      "2024-02-15 19:10:34,007\tINFO actor_pool_map_operator.py:114 -- MapBatches(TorchPredictor): Waiting for 2 pool actors to start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bd24ecc90b4b77bc90402f99f59887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5834)\u001b[0m /Users/luweizheng/miniconda3/envs/dispy/lib/python3.11/site-packages/ray/data/_internal/arrow_block.py:148: FutureWarning: promote has been superseded by mode='default'.\n",
      "\u001b[36m(ReadParquet->SplitBlocks(50) pid=5834)\u001b[0m   return transform_pyarrow.concat(tables)\n",
      "2024-02-15 19:10:35,525\tWARNING actor_pool_map_operator.py:278 -- To ensure full parallelization across an actor pool of size 2, the Dataset should consist of at least 2 distinct blocks. Consider increasing the parallelism when creating the Dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'output': 3.4000000953674316},\n",
       " {'output': 3.4000000953674316},\n",
       " {'output': 10.199999809265137}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TorchPredictor:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torch.nn.Identity()\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        pred = {}\n",
    "        inputs = torch.as_tensor(df['trip_distance'], dtype=torch.float32)\n",
    "        with torch.inference_mode():\n",
    "            pred[\"output\"] = self.model(inputs).detach().numpy()\n",
    "        return pred\n",
    "\n",
    "pred_ds = batch_ds.limit(100).map_batches(TorchPredictor, compute=ray.data.ActorPoolStrategy(size=2))\n",
    "pred_ds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using actors typically involves three steps:\n",
    "\n",
    "1. Create a class that includes an `__init__()` method and a `__call__()` method. The `__init__()` method initializes state data, and the `__call__()` method implements the transformation operation. Refer to the previously implemented `TorchPredictor` class for reference.\n",
    "2. Create an `ActorPoolStrategy`, specifying the total number of workers.\n",
    "3. Call the `map_batch()` method, passing the `ActorPoolStrategy` to the `compute` parameter.\n",
    "\n",
    "## Grouping\n",
    "\n",
    "Another frequently used primitive in data processing is grouping and aggregation. Ray Data provides [groupby()](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.groupby.html). Ray Data first uses `groupby()` to group the data based on certain fields, and then uses [`map_groups()`](https://docs.ray.io/en/latest/data/api/doc/ray.data.grouped_data.GroupedData.map_groups.html) to aggregate the grouped data.\n",
    "\n",
    "The `groupby(key)` parameter `key` specifies the field for grouping, and the `map_groups(fn)` parameter `fn` operates on data within the same group. Ray Data provides some predefined aggregation functions such as `sum()`, `max()`, `mean()`, etc. In the following example, `mean()` is used to aggregate the `value` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 19:10:35,568\tINFO streaming_executor.py:112 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[Aggregate] -> LimitOperator[limit=20]\n",
      "2024-02-15 19:10:35,568\tINFO streaming_executor.py:113 -- Execution config: ExecutionOptions(resource_limits=ExecutionResources(cpu=None, gpu=None, object_store_memory=None), exclude_resources=ExecutionResources(cpu=0, gpu=0, object_store_memory=0), locality_with_output=False, preserve_order=False, actor_locality_enabled=True, verbose_progress=False)\n",
      "2024-02-15 19:10:35,568\tINFO streaming_executor.py:115 -- Tip: For detailed progress reporting, run `ray.data.DataContext.get_current().execution_options.verbose_progress = True`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23993e68c2e648afb92e8991dac13432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "- Aggregate 1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9d3d562d3d401394b0765509a65d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Map 2:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550fbe5005124eaf8061fed383ccf0fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffle Reduce 3:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6294dea3a604962bb3cfe99142d215b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "834c71fc3638419985a73da547144b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sort Sample 0:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'group': 1, 'mean(value)': 1.5}\n",
      "{'group': 2, 'mean(value)': 3.5}\n"
     ]
    }
   ],
   "source": [
    "ds = ray.data.from_items([\n",
    "    {\"group\": 1, \"value\": 1},\n",
    "    {\"group\": 1, \"value\": 2},\n",
    "    {\"group\": 2, \"value\": 3},\n",
    "    {\"group\": 2, \"value\": 4}])\n",
    "mean_ds = ds.groupby(\"group\").mean(\"value\")\n",
    "mean_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dispy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
